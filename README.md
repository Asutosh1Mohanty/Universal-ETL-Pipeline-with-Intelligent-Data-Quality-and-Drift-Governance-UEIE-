Project: Universal ETL Pipeline with Automated Data Quality Checks

This project is an end-to-end Data Engineering ETL pipeline that can process ANY dataset you upload (CSV, Excel, JSON, Parquet).
It performs:
   * Extraction from local file 
   * Automatic cleaning 
   * Column standardization 
   * Missing value handling 
   * Date parsing 
   * Duplicate removal 
   * Automated data quality (DQ) checks 
   * Logging
   * And saves clean outputs + quality reports
It is designed to imitate real-world ingestion pipelines used in companies.

ğŸ¯ Goal of the Project
The goal is to create a reusable, production-inspired ETL pipeline that can:
   âœ” Accept any raw dataset
   âœ” Standardize and clean data
   âœ” Validate data quality
   âœ” Generate structured and human-readable reports
   âœ” Provide logs for traceability
   âœ” Serve as Step 1 of a full Data Engineering portfolio

This project becomes the foundation for future steps like:
   * PostgreSQL Data Warehouse
   * Airflow scheduling
   * PySpark transformations
   * AWS Data Lake 
   * Redshift Warehouse

ğŸ—ï¸ High-Level Architecture
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚           RAW DATA (any file)       â”‚
             â”‚    CSV / Excel / JSON / Parquet     â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚              Extract                 â”‚
             â”‚  - Detect latest file in /raw        â”‚
             â”‚  - Load CSV/Excel/JSON/Parquet       â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚             Transform                â”‚
             â”‚  - Standardize columns               â”‚
             â”‚  - Drop duplicates                   â”‚
             â”‚  - Type conversions                  â”‚
             â”‚  - Missing value handling            â”‚
             â”‚  - Parse date-like columns           â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                Load                  â”‚
             â”‚     Save cleaned CSV/Parquet         â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚          Data Quality Check          â”‚
             â”‚  - Null counts                       â”‚
             â”‚  - Duplicate records                 â”‚
             â”‚  - Date parsing issues               â”‚
             â”‚  - Numeric validation                â”‚
             â”‚  - PASS/FAIL verdict                 â”‚
             â”‚  - TXT + JSON reports                â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                Logs                  â”‚
             â”‚        logs/pipeline.log            â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸš€ Features
âœ” Accepts ANY data file
Supports:
   * .csv
   * .xlsx
   * .xls
   * .json
   * .parquet

âœ” Automatic column standardization
Converts:
   * "Order Date" â†’ order_date
   * "Unit-Price" â†’ unit_price

âœ” Smart cleaning
   * Remove duplicates
   * Convert numeric columns
   * Parse date-like columns
   * Handle missing values based on data type

âœ” Automated data quality checks
The pipeline auto-checks:
   * Null counts 
   * Duplicate rows 
   * Non-numeric numeric values 
   * Negative values 
   * Unparsable dates 
   * Column type consistency

Generates:
   * processed/quality_report.txt (readable)
   * processed/quality_report.json (machine-friendly)

âœ” Logging
All activity is logged in:
   * logs/pipeline.log    

âœ” Chunk support for HUGE CSV files
Automatically processes datasets in chunks for memory safety.

ğŸ§° Tech Stack
Component	            Tools Used
Language	            Python 3.10+
Data Processing	        Pandas
Formats	                CSV / Excel / JSON / Parquet
Logging	                Python logging module
Testing	                (Optional) pytest
Storage	                Local filesystem 
Metadata	            JSON DQ report

ğŸ“‚ Project Structure
Data_Engineering_Projects/
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py       # Load raw data (any file type)
â”‚   â”œâ”€â”€ transform.py     # Clean, standardize, parse dates, fix types
â”‚   â”œâ”€â”€ load.py          # Save cleaned outputs
â”‚   â””â”€â”€ quality.py       # Perform data quality checks
â”‚
â”œâ”€â”€ raw/                 # Place your input file here
â”œâ”€â”€ processed/           # Cleaned output + quality reports
â”œâ”€â”€ logs/                # Pipeline logs
â”œâ”€â”€ main.py              # Orchestrator
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ› ï¸ How the ETL Pipeline Works
1) Extraction
      * Locate the latest file inside the raw/ folder. 
      * Load file depending on type:
          * pd.read_csv()
          * pd.read_excel()
          * pd.read_json()
          * pd.read_parquet()
      * If CSV is huge â†’ optionally load in chunks.

2) Transformation
The following operations are applied:
      * Column Standardization 
      * Lowercase
      * Remove spaces and hyphens
      * Uniform naming
      * Duplicate Removal
              df = df.drop_duplicates().reset_index(drop=True)
      * Missing Value Handling 
      * Numeric column â†’ fill with 0
      * Object column â†’ fill with "unknown"
      * Datetime column â†’ keep NaT for DQ check
      * Convert invalid numbers using errors='coerce'
      * Auto Date Parsing
             Column names containing "date" or "time" are converted using:
                   pd.to_datetime(df[col], errors="coerce")

3) Loading
Saves cleaned data to:
     processed/cleaned_sales.csv
(or Parquet version if selected)

4) Data Quality Checks
Checks include:

Check	              Description
Null Count	          Count missing per column
Duplicate Rows	      Identify duplicate records
Numeric validation	  Check non-numeric or negative values
Date parsing	      How many values failed to convert
Uniqueness	          Detect duplicate IDs
Verdict	              PASS/FAIL based on rules

Outputs:
   * quality_report.txt 
   * quality_report.json

5) Logging
Every run logs:
     * Timestamp 
     * Steps executed
     * Warnings
     * Errors 
     * Quality verdict

Example:
      2025-02-01 10:21:22 INFO Pipeline started
      2025-02-01 10:21:22 INFO Loaded raw dataframe with 10 rows
      2025-02-01 10:21:22 INFO Saved cleaned data to processed/cleaned_sales.csv
      2025-02-01 10:21:22 INFO Quality verdict: PASS

â–¶ï¸ How to Use the Pipeline
1. Clone repo
      git clone <your-repo-url>
      cd Data_Engineering_Projects

2. Create venv
      python -m venv venv
      venv\Scripts\activate        # Windows
      source venv/bin/activate     # Mac/Linux

3. Install dependencies
       pip install -r requirements.txt

4. Put your input file 
        Place any file inside:
            raw/yourdata.xlsx
            raw/sales.csv
            raw/data.json
            raw/bigfile.parquet

5. Run
       python main.py

6. Check output
        * processed/cleaned_sales.csv 
        * processed/quality_report.txt
        * processed/quality_report.json
        * logs/pipeline.log

ğŸ§ª Example Scenario
If you upload:
raw/Sales_Q1_2024.xlsx

The pipeline will:
      âœ” Detect this file
      âœ” Load Excel
      âœ” Standardize columns
      âœ” Remove duplicates
      âœ” Parse date columns
      âœ” Convert volume/price columns
      âœ” Save cleaned CSV
      âœ” Generate a DQ report
      âœ” Log everything
You donâ€™t need to rename the file â€” any name works.

ğŸŒŸ Why this Project is Impressive for Data Engineering
This project demonstrates:
      * Ingestion from heterogeneous data sources 
      * ETL modularization 
      * Data quality management
      * File detection and metadata handling
      * Error handling
      * Logging & reporting 
      * Clean project structure

These are exactly the skills tested in:
      * Data Engineer interviews
      * Data Platform teams
      * ML Ops teams
      * ETL / Analytics engineering roles

ğŸ“ Resume Bullet Point
Built a universal Python-based ETL pipeline capable of ingesting CSV/Excel/JSON/Parquet files, performing automated data cleaning, type normalization, date parsing, duplicate removal, and generating detailed text/JSON data-quality reports with full logging support.