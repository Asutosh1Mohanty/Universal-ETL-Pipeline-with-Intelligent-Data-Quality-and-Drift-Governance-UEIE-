Project: Universal ETL Pipeline with Intelligent Data Quality and Drift Governance (UEIE)

This project is an production-inspired, intelligent Data Engineering ETL pipeline designed that ingest ANY dataset (CSV, Excel, JSON, Parquet).
It automatically performs:
    * Data extraction 
    * Cleaning & standardization 
    * Automated data quality checks 
    * Intelligent quality scoring 
    * Quality gating (PASS / WARN / FAIL)
    * Data drift detection across runs 
    * Drift severity classification
    * Auto-fail protection for critical drift
    * Full execution metadata & lineage
    * Centralized logging
It is designed to imitate real-world data ingestion pipelines used in modern data platforms.

ğŸ¯ Goal of the Project
The goal is to build a reusable, extensible, and governed ETL foundation that can:
    âœ” Accept any raw dataset without manual schema definition
    âœ” Automatically clean and standardize data
    âœ” Validate data quality with scoring & rules
    âœ” Detect silent data drift across pipeline runs
    âœ” Prevent bad or breaking data from flowing downstream
    âœ” Generate machine-readable metadata and human-readable reports
    âœ” Serve as the foundation for advanced data platforms

This project becomes the foundation for future steps like:
   * PostgreSQL Data Warehouse
   * Airflow scheduling
   * PySpark transformations
   * AWS Data Lake 
   * Redshift Warehouse

Key Intelligence: UEIE (Universal ETL Intelligence Engine)
UEIE is a custom-built intelligence layer added on top of the ETL pipeline that provides:
    * Dataset profiling 
    * Rule inference 
    * Quality scoring
    * Quality gates
    * Drift detection
    * Drift severity classification
    * Run-level metadata & lineage
This makes the pipeline decision-aware, not just procedural.

ğŸ—ï¸ High-Level Architecture
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚           RAW DATA (any file)       â”‚
             â”‚    CSV / Excel / JSON / Parquet     â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚              Extract                â”‚
             â”‚  - Detect latest file in /raw       â”‚
             â”‚  - Load CSV/Excel/JSON/Parquet      â”‚
             â”‚  - Chunk large CSVs safely          â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚             Transform               â”‚
             â”‚  - Standardize columns              â”‚
             â”‚  - Drop duplicates                  â”‚
             â”‚  - Type conversions                 â”‚
             â”‚  - Missing value handling           â”‚
             â”‚  - Parse date-like columns          â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                Load                 â”‚
             â”‚     Save cleaned CSV/Parquet        â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚       Data Quality & UEIE Layer     â”‚
             â”‚  - Quality Checks                   â”‚
             â”‚  - Quality Score                    â”‚
             â”‚  - PASS / WARN / FAIL gate          â”‚
             â”‚  - Drift Detection                  â”‚
             â”‚  - Drift Severity                   â”‚
             â”‚   (MINOR/MAJOR/CRITICAL)            â”‚
             â”‚  - Auto-fail on CRITICAL drift      â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚      Metadata, Lineage and Logs     â”‚
             â”‚   - run_metadata.json               â”‚
             â”‚   - last_profile.json               â”‚
             â”‚   - logs/pipeline.log               â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸš€ Features
âœ” Universal File Ingestion
Supports:
   * .csv
   * .xlsx
   * .xls
   * .json
   * .parquet
No file renaming required

âœ” Automatic column standardization
Converts:
   * "Order Date" â†’ order_date
   * "Unit-Price" â†’ unit_price
Ensures consistent downstream processing

âœ” Smart Data Cleaning
   * Duplicate removal 
   * Numeric coercion 
   * Date parsing (auto-detected via column names)
   * Missing value handling by data type 
   * Safe handling of invalid values

âœ” Automated Data Quality Checks
The pipeline auto-checks:
   * Null counts per column 
   * Duplicate records 
   * Numeric validation (non-numeric, negative values)
   * Date parsing failures 
   * Uniqueness checks 
   * Schema consistency

Generates / Outputs:
   * processed/quality_report.txt (human-readable)
   * processed/quality_report.json (machine-readable)

âœ” UEIE Quality Scoring
Each dataset receives a quality score (0â€“100) based on profiling results.

âœ” Quality Gate Enforcement (Governance)
Quality Score   |   Status  |   Behavior
â‰¥ 80	        |   PASS    |	Pipeline continues
60â€“79	        |   WARN    |	Pipeline continues with warning
< 60    	    |   FAIL    |   Pipeline stops
Prevents bad data from reaching downstream systems.

âœ” Data Drift Detection (Across Runs)
Automatically compares:
    * Null percentage
    * Uniqueness percentage
    * Data types
    * Schema changes
    * Column additions/removals
Uses previous run profile as baseline.

âœ” Drift Severity Classification
Detected drift is classified as:
    * MINOR â€“ small statistical changes
    * MAJOR â€“ significant distribution shifts
    * CRITICAL â€“ schema/type changes or column removal

âœ” Auto-Fail on Critical Drift
If CRITICAL drift is detected:
    * Pipeline fails immediately 
    * Downstream systems are protected
This mimics enterprise data governance rules.

âœ” Production-Grade Logging
All activity is logged in:
   * logs/pipeline.log
Includes:
   * Execution steps
   * Warnings
   * Errors
   * Quality decisions
   * Drift alerts

âœ” Chunk support for HUGE CSV files
Automatically processes datasets in chunks for memory safety.

ğŸ§° Tech Stack
Component	            Tools Used
Language	            Python 3.10+
Data Processing	        Pandas
Formats	                CSV / Excel / JSON / Parquet
Logging	                Python logging
Governance	            Custom UEIE engine
Storage	                Local filesystem 
Metadata	            JSON

ğŸ“‚ Project Structure
Data_Engineering_Projects/
â”‚
â”œâ”€â”€ etl/                         # Core ETL logic
â”‚   â”œâ”€â”€ extract.py               # Load raw data (CSV / Excel / JSON / Parquet)
â”‚   â”œâ”€â”€ transform.py             # Clean, standardize columns, parse dates, fix types
â”‚   â”œâ”€â”€ load.py                  # Save cleaned outputs (CSV / Parquet)
â”‚   â””â”€â”€ quality.py               # Perform automated data quality checks
â”‚
â”œâ”€â”€ ueie/                        # Universal ETL Intelligence Engine (governance layer)
â”‚   â”œâ”€â”€ detector.py              # Detect file metadata (name, size, extension)
â”‚   â”œâ”€â”€ profiler.py              # Profile dataset (null %, uniqueness, dtypes)
â”‚   â”œâ”€â”€ rule_engine.py           # Infer quality rules from profile
â”‚   â”œâ”€â”€ scorer.py                # Calculate overall data quality score
â”‚   â”œâ”€â”€ gate.py                  # PASS / WARN / FAIL quality gate
â”‚   â”œâ”€â”€ drift.py                 # Detect data drift across pipeline runs
â”‚   â”œâ”€â”€ drift_gate.py            # Classify drift severity & enforce governance
â”‚   â””â”€â”€ metadata.py              # Store run metadata & profile history
â”‚
â”œâ”€â”€ raw/                         # Place your input data files here
â”‚                                 # (CSV / Excel / JSON / Parquet)
â”‚
â”œâ”€â”€ processed/                   # Pipeline outputs & metadata
â”‚   â”œâ”€â”€ cleaned_*.csv             # Cleaned datasets
â”‚   â”œâ”€â”€ quality_report.txt        # Human-readable quality report
â”‚   â”œâ”€â”€ quality_report.json       # Machine-readable quality report
â”‚   â”œâ”€â”€ run_metadata.json         # Run-level execution metadata
â”‚   â””â”€â”€ last_profile.json         # Previous run profile (for drift detection)
â”‚
â”œâ”€â”€ logs/                        # Pipeline execution logs
â”‚   â””â”€â”€ pipeline.log
â”‚
â”œâ”€â”€ main.py                      # Pipeline orchestrator (entry point)
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # Project documentation


ğŸ› ï¸ How the ETL Pipeline Works
1) Extraction
      * Locate the latest file inside the raw/ folder. 
      * Load file depending on type:
          * pd.read_csv()
          * pd.read_excel()
          * pd.read_json()
          * pd.read_parquet()
      * If CSV is huge â†’ optionally load in chunks.

2) Transformation
The following operations are applied:
      * Column Standardization 
      * Lowercase
      * Remove spaces and hyphens
      * Uniform naming
      * Duplicate Removal
              df = df.drop_duplicates().reset_index(drop=True)
      * Missing Value Handling 
      * Numeric column â†’ fill with 0
      * Object column â†’ fill with "unknown"
      * Datetime column â†’ keep NaT for DQ check
      * Convert invalid numbers using errors='coerce'
      * Auto Date Parsing
             Column names containing "date" or "time" are converted using:
                   pd.to_datetime(df[col], errors="coerce")

3) Loading
Saves cleaned data to:
     processed/cleaned_sales.csv
(or Parquet version if selected)

4) Data Quality Checks
Checks include:

Check	              Description
Null Count	          Count missing per column
Duplicate Rows	      Identify duplicate records
Numeric validation	  Check non-numeric or negative values
Date parsing	      How many values failed to convert
Uniqueness	          Detect duplicate IDs
Verdict	              PASS/FAIL based on rules

Outputs:
   * quality_report.txt 
   * quality_report.json

5) Logging
Every run logs:
     * Timestamp 
     * Steps executed
     * Warnings
     * Errors 
     * Quality verdict

Example:
        2025-12-20 13:10:36 INFO Pipeline started
        2025-12-20 13:10:36 INFO Rows processed: 10000
        2025-12-20 13:10:36 INFO UEIE quality score: 100.0
        2025-12-20 13:10:36 INFO UEIE quality gate status: PASS
        2025-12-20 13:10:36 INFO Pipeline finished


â–¶ï¸ How to Use the Pipeline
1. Clone repo
      git clone <your-repo-url>
      cd Data_Engineering_Projects

2. Create venv
      python -m venv venv
      venv\Scripts\activate        # Windows
      source venv/bin/activate     # Mac/Linux

3. Install dependencies
       pip install -r requirements.txt

4. Put your input file 
        Place any file inside:
            raw/yourdata.xlsx
            raw/sales.csv
            raw/data.json
            raw/bigfile.parquet

5. Run
       python main.py

6. Check output
        * processed/cleaned_sales.csv 
        * processed/quality_report.txt
        * processed/quality_report.json
        * processed/run_metadata.json
        * logs/pipeline.log

ğŸ§ª Example Scenario
If you upload:
customers.csv

The pipeline will:
        âœ” Detect the file automatically
        âœ” Cleans and standardize the data
        âœ” Runs data quality checks
        âœ” Calculate a quality score
        âœ” Apply quality gate rules
        âœ” Detect data drift vs previous run
        âœ” Enforce drift severity governance 
        âœ” Saves cleaned data, reports and metadata
        âœ” Logs all decisions
You donâ€™t need to rename the file â€” any name works.

ğŸŒŸ Why this Project is Impressive for Data Engineering
This project demonstrates:
        * Ingestion from heterogeneous data sources 
        * Modular ETL design
        * Automated data quality enforcement 
        * Intelligent quality gating
        * Data drift detection & governance
        * Metadata & lineage tracking
        * Production-grade logging
        * Clean, scalable project architecture

These are exactly the skills tested in:
        * Data Engineer
        * Analytics Engineer
        * Data Platform teams
        * ML Ops teams
        * ETL / Analytics engineering roles

ğŸ“ Resume Bullet Point
Built a universal, intelligence-driven ETL pipeline in Python capable of ingesting heterogeneous data formats, performing automated cleaning, quality scoring, quality gating, drift detection with severity classification, and enforcing governance rules with full execution metadata and logging.
